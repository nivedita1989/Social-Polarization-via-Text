{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "def clean_text(text):\n",
    "    text=\" \"+text+\" \"\n",
    "    text=text.replace(\"Äôs\",\" us\")#change org appeal specific typing error\n",
    "    text=text.replace(\"don‚Äôt\",\" don't\")#change org appeal specific typing error\n",
    "    puncts=string.punctuation\n",
    "    for punct in puncts:\n",
    "        if(punct!=\"'\"):\n",
    "            s=\" \"+punct+\" \"\n",
    "            text=text.replace(punct,s)\n",
    "    text=text.replace(\"ain't\",\"are not\")\n",
    "    text=text.replace(\"cannot\",\"can not\")\n",
    "    text=text.replace(\"havn't\",\"have not\")\n",
    "    text=text.replace(\"n't\",\" not\")\n",
    "    text=re.sub(r'\\s+',' ',text)\n",
    "    text=re.sub(r'\\S*@\\S*\\s?',' ',text)#emails remove\n",
    "    text=re.sub(r'http\\S+',' ',text)#hyperlinks \n",
    "    text=re.sub(r' \\d+',' ',text)#numbers\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet, sentiwordnet\n",
    "\n",
    "NEGATORS = [\"ain't\",'cannot',\"can't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",'hardly',\n",
    "            \"hasn't\",\"haven't\",\"havn't\",\"isn't\",'lack','lacking','lacks','neither',\n",
    "            'never','no','nobody','none','nor','not','nothing','nowhere',\"mightn't\",\n",
    "            \"mustn't\",\"needn't\",\"oughtn't\",\"shouldn't\",\"wasn't\",'without',\"wouldn't\",\n",
    "            'are not','can not', 'did not','does not','do not','had not', 'has not',\n",
    "            'have not','is not','might not','must not','need not','ought not','should not',\n",
    "            'was not','would not', \"n't\"]\n",
    "NEGATORS.sort(key = lambda x: 1/len(x))\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all node names\n",
    "entity_keys=nodesID.values()\n",
    "\n",
    "#gives unique nodes in a sentence from NewText\n",
    "def uniqueentitylist(phrase,entity_keys):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
    "    phraseTokens=list()\n",
    "    phraseTokens=tokenizer.tokenize(phrase)\n",
    "    unique_entity_set=set()\n",
    "    unique_entity_list=[t for t in phraseTokens if t in entity_keys ]\n",
    "    unique_entity_set.update(unique_entity_list)\n",
    "    return list(unique_entity_set)\n",
    "\n",
    "#counts no of unique nodes in a sentence from NewText\n",
    "def uniqueentitycount(phrase,entity_keys):\n",
    "    return len(uniqueentitylist(phrase,entity_keys))\n",
    "\n",
    "#gives pair of unique nodes per sentence \n",
    "def entity_pairs_in_sent(l):\n",
    "    l.sort()\n",
    "    pairs=list()\n",
    "    for i in range(len(l)):\n",
    "        x=list()\n",
    "        y=list()\n",
    "        for j in range(i+1,len(l)):\n",
    "            x.append(l[i])\n",
    "            y.append(l[j])\n",
    "        pairs.extend(zip(x,y))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(phrase_list,sentID,depAnn):\n",
    "    tup_lists=list()\n",
    "    for phrase in phrase_list:\n",
    "        tup_list=list()\n",
    "        for i in range(0,len(ann.sentence[sentID].token)):\n",
    "            if depAnn.sentence[sentID].token[i].word in phrase:\n",
    "                tup_list.append((depAnn.sentence[sentID].token[i].word,depAnn.sentence[sentID].token[i].pos))\n",
    "        tup_lists.append(tup_list)\n",
    "    return tup_lists\n",
    "\n",
    "\"\"\"with CoreNLPClient(properties={'annotators': 'coref', 'coref.algorithm' : 'statistical'}, timeout=60000, memory='16G') as client:\n",
    "    dp_ann = client.annotate(text)\n",
    "    chains = dp_ann.corefChain\"\"\"\n",
    "def toROOTparse(dependencyAnn,sentID,x):\n",
    "    snt=dependencyAnn.sentence[sentID]\n",
    "    dependencyParse=snt.basicDependencies\n",
    "    dp=dependencyParse\n",
    "    upwardParse=dict()\n",
    "    for e in dp.edge:\n",
    "        upwardParse[snt.token[e.target-1].word]=snt.token[e.source-1].word\n",
    "        \n",
    "    [i]=dp.root\n",
    "    root_word=snt.token[i-1].word\n",
    "    if(x==\"Node_1\"):\n",
    "        refs=node1[sentID]\n",
    "    elif(x==\"Node_2\"):\n",
    "        refs=node2[sentID]\n",
    "    elif(x==\"Node_3\"):\n",
    "        refs=node3[sentID]\n",
    "    elif(x==\"Node_4\"):\n",
    "        refs=node4[sentID]\n",
    "    elif(x==\"Node_5\"):\n",
    "        refs=node5[sentID]\n",
    "    elif(x==\"Node_6\"):\n",
    "        refs=node6[sentID]\n",
    "    phrasefortokens=list()\n",
    "    for ref in refs:\n",
    "        refTokens=tokenizer.tokenize(ref)\n",
    "        anspath=list()\n",
    "        for r in refTokens:\n",
    "            refpath=list()\n",
    "            x=r\n",
    "            while x!=root_word:\n",
    "                refpath.append(upwardParse[x])\n",
    "                x=upwardParse[x]\n",
    "            refpath.append(root_word)\n",
    "            if(len(refpath)>len(anspath)):\n",
    "                anspath=refpath\n",
    "        phrasefortokens.append(anspath)\n",
    "    return phrasefortokens\n",
    "\n",
    "\"\"\"for a sentence, given two entities, we access how\n",
    "the entity is referenced in the sentence and get a bridge of words connecting both\n",
    "entities in the dependency parse--- a list all such bridge words for the no of references\n",
    "of each entity ---a list of tuples with words and pos tags\"\"\"\n",
    "def findDependencyPhrase(sentID,x,y,dp_ann):\n",
    "    pthlists=list()\n",
    "    paths_x=toROOTparse(dp_ann,sentID,x)#a list of paths for all references\n",
    "    paths_y=toROOTparse(dp_ann,sentID,y)#a list of paths for all references\n",
    "    for px in paths_x:#px is a list of words\n",
    "        pth=list()\n",
    "        for py in paths_y:#py is a list of words\n",
    "            for xword in px:\n",
    "                xword=xword.lower()\n",
    "                for yword in py:\n",
    "                    yword=yword.lower()\n",
    "                    if(xword==yword):\n",
    "                        key=yword\n",
    "                        break\n",
    "            for xword in px:\n",
    "                if xword==key:\n",
    "                    break\n",
    "                else: \n",
    "                    pth.append(xword)\n",
    "            for yword in py:\n",
    "                if yword==key:\n",
    "                    break\n",
    "                else: \n",
    "                    pth.append(yword)\n",
    "            pth.append(key)\n",
    "        pthlists.append(pth)\n",
    "    return get_pos(pthlists,sentID,dp_ann)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('started', 'VBD'),\n",
       "  ('Nations', 'NNP'),\n",
       "  ('SORBOJOYA', 'NN'),\n",
       "  ('project', 'NN'),\n",
       "  ('assure', 'VB')]]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_ann=ann\n",
    "sentID=2\n",
    "x=\"Node_4\"\n",
    "y=\"Node_5\"\n",
    "ll=findDependencyPhrase(sentID,x,y,dp_ann)\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_sentiment(word, pos=None):\n",
    "    \"\"\"\n",
    "    Given a word and an optional part of speech tag,\n",
    "    returns the average sentiment score for that word using senti_synsets.\n",
    "    \"\"\"\n",
    "    if word in STOP_WORDS or word in NEGATORS:\n",
    "        return 0\n",
    "\n",
    "    if pos == None:\n",
    "        synsets = list(sentiwordnet.senti_synsets(word))\n",
    "\n",
    "    elif pos != None:\n",
    "        def get_wordnet_pos(tag):\n",
    "            \"\"\"\n",
    "            Translating Spacy's POS tags to WordNet's POS tags\n",
    "            https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-label\n",
    "            \"\"\"\n",
    "            if tag.startswith('JJ'): return wordnet.ADJ\n",
    "            elif tag.startswith('VB'): return wordnet.VERB\n",
    "            elif tag.startswith('NN'): return wordnet.NOUN\n",
    "            elif tag.startswith('RB'): return wordnet.ADV\n",
    "            else: return wordnet.NOUN\n",
    "\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        synsets = list(sentiwordnet.senti_synsets(word.lower(), wordnet_pos))\n",
    "\n",
    "    score = 0\n",
    "    for el in synsets:\n",
    "        score += el.pos_score()\n",
    "        score -= el.neg_score()\n",
    "\n",
    "    if score == 0 and pos != None:\n",
    "        return word_to_sentiment(word, pos=None)\n",
    "\n",
    "    if len(synsets) == 0: return 0\n",
    "\n",
    "    score /= len(synsets)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_sentiment(pos_tags):\n",
    "    \"\"\"\n",
    "    Finds the average sentiment of a list of tuples of (word, pos) using word_to_sentiment()\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    words_with_sentiment = 0\n",
    "    for tup in pos_tags:\n",
    "        word, pos = tup[0], tup[1]\n",
    "        if word_to_sentiment(word, pos) != 0:\n",
    "            words_with_sentiment += 1\n",
    "            score += word_to_sentiment(word.lower(), pos)\n",
    "    if score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score/words_with_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload text file\n",
    "path='/Volumes/Seagate Expansion Drive/Minnesota /sentiment analysis/gender sample 1'\n",
    "with open(path,'r') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "text=clean_text(text)#cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-4e1774436e664ab7.props -preload coref\n",
      "women 's of our country <-> themselves\n",
      "their <-> They <-> Middle aged women\n",
      "them <-> every women of Bangladesh <-> their\n",
      "us <-> our <-> our <-> our <-> we\n",
      "their <-> still women\n",
      "women <-> their\n",
      "% women <-> the women <-> the women\n",
      "our country <-> our country <-> Bangladesh <-> our country <-> Bangladesh <-> Bangladesh <-> Bangladesh <-> Bangladesh <-> Bangladesh <-> the country <-> Bangladesh\n",
      "the women 's of Bangladesh <-> their <-> Women from different sectors\n",
      "them <-> their\n",
      "SORBOJOYA <-> SORBOJOYA <-> United Nations and other \" SORBOJOYA\n",
      "its <-> it\n",
      "its mother <-> the mother\n",
      "the baby <-> The baby\n",
      "it <-> using sanitary napkin\n",
      "our <-> Our <-> Our <-> we <-> we\n",
      "their <-> Women of rural areas , Rohingya women , tribal women <-> They <-> They\n",
      "this huge population behind <-> the total population <-> a half crore population of Bangladesh\n",
      "this cancer <-> Prostate cancer\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"!pip install stanza\n",
    "import stanza\n",
    "!echo \"Downloading CoreNLP...\"\n",
    "!wget \"http://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip\" -O corenlp.zip\n",
    "!unzip corenlp.zip\n",
    "!mv ./stanford-corenlp-4.0.0 ./corenlp\n",
    "\n",
    "# Set the CORENLP_HOME environment variable to point to the installation location\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = \"./corenlp\"\n",
    "from stanza.server import CoreNLPClient\"\"\"\n",
    "#coreference resolution\n",
    "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
    "with CoreNLPClient(properties={'annotators': 'coref', 'coref.algorithm' : 'statistical'}, timeout=60000, memory='16G') as client:\n",
    "    ann = client.annotate(text)\n",
    "    mychains = list()\n",
    "    chains = ann.corefChain\n",
    "    for chain in chains:\n",
    "        mychain = list()\n",
    "        # Loop through every mention of this chain\n",
    "        for mention in chain.mention:\n",
    "            # Get the sentence in which this mention is located, and get the words which are part of this mention\n",
    "            # (we can have more than one word, for example, a mention can be a pronoun like \"he\", but also a compound noun like \"His wife Michelle\")\n",
    "            words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
    "            #build a string out of the words of this mention\n",
    "            ment_word = ' '.join([x.word for x in words_list])\n",
    "            mychain.append(ment_word)\n",
    "        mychains.append(mychain)\n",
    "\n",
    "for chain in mychains:\n",
    "    print(' <-> '.join(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prostate cancer'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ment_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have signed . Let ‚  Node_1  get to , ! Mahfuz Islam started this petition to  Node_5       \" is a project to assure women health service in  Node_4  . The purpose of  Node_5  is to provide safe menstrual health service to teenagers , tribal , Rohingya women and women of rural areas . Women plays a vital role towards  Node_1  nation . Nine and  Node_4       is women which is more than half of  Node_4    . So , the progress of  Node_4   is not possible by keeping  Node_4     . That 's the reason  Node_1  need to focus on women 's health and need to keep  Node_1  women 's safe . In  Node_4   Node_3   are ashamed to discuss on  Node_3  menstrual health .  Node_3           are unaware of safe menstrual health service .  Node_3  use clothes instead of sanitary napkins . Clothes are very unhygienic and unfriendly to skin .  Node_3  do not have any idea about safe sanitation and menstrual health . Then women who have less earning or striving in  Node_3  day to day life ca not afford a costly sanitary napkin . By this way  Node_3     Node_4   derive  Node_3  from safe sanitation . This ignorance of  Node_3  towards  Node_3  health results to maternal problems and diseases . Diseases like Prostate Cancer , Cysts , Uterus Cancer , etc takes place in women 's body . In  Node_4  percent of the women affected by cancer sufferers from Prostate  . In each minutes woman gets affected by Prostate cancer and women are identified with the symptoms of this  every year . women dies each year due to this disease . It 's seen that most women dies are the sufferer of menstrual diseases . When a woman dies in a family gets affected , a society gets affected , moreover a nation gets affected . Day by day orphans are growing in  Node_4   due to the death of  Node_3   . When a lactating mother dies , it also affects  Node_6   .  Node_6   is derived from having the milk and care from  Node_3   . This affects the overall growth of the child too .  Node_3    or the teenagers suffers a lot due to this diseases .  Node_3  need to leave  Node_3  education half way . Some who gets married ca not adopt child .  Node_3     lack on  Node_3  day to day life . This scenario of  Node_3      is so shacking . So being an developing country this phenomenon is not acceptable at all . So  Node_5  is trying to eliminate this unhygienic from the society . At present about  Node_3   do not know about using   and % of women knows about it but are not concerned . Only % of  Node_3   can afford the sanitary napkins .  Node_1  aim is to reach  Node_3     Node_4  and educate  Node_3  about safe menstrual health and the importance of using sanitary napkins during  Node_3  period . If proper sanitation can be assured among  Node_3   then the percentage of diseases and the percentage of women dying due to menstrual disease can be lessened to % .  Node_1  goal is to aware women about  Node_3  health and make sanitary napkin available to  Node_3  . By this  Node_1  can save a huge population of  Node_1   which is a human resource . For this purpose  Node_1  want your support to take this project further and make life better for all the women of  Node_4  .\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Changes annotated text\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chains = ann.corefChain\n",
    "for chain in chains:\n",
    "    for mention in chain.mention:\n",
    "        indexrange=range(mention.beginIndex,mention.endIndex)\n",
    "        for v in indexrange:\n",
    "            if(v==mention.beginIndex):\n",
    "                if chain.chainID in nodesfromchainID[\"Node_1\"]:\n",
    "                    ann.sentence[mention.sentenceIndex].token[v].word=' Node_1 '\n",
    "                elif chain.chainID in nodesfromchainID[\"Node_3\"]:\n",
    "                    ann.sentence[mention.sentenceIndex].token[v].word=' Node_3 '\n",
    "                elif chain.chainID in nodesfromchainID[\"Node_4\"]:\n",
    "                    ann.sentence[mention.sentenceIndex].token[v].word=' Node_4 '\n",
    "                elif chain.chainID in nodesfromchainID[\"Node_5\"]:\n",
    "                    ann.sentence[mention.sentenceIndex].token[v].word=' Node_5 '\n",
    "                elif chain.chainID in nodesfromchainID[\"Node_6\"]:\n",
    "                    ann.sentence[mention.sentenceIndex].token[v].word=' Node_6 '\n",
    "            else:\n",
    "                ann.sentence[mention.sentenceIndex].token[v].word=''\n",
    "sentences=list()\n",
    "for sent in ann.sentence:\n",
    "    words=list()\n",
    "    for tok in sent.token:\n",
    "        words.append(tok.word)\n",
    "    sentence=' '.join(words)\n",
    "#    print(sentence)\n",
    "    sentences.append(sentence)\n",
    "NewText=' '.join(sentences)\n",
    "print(NewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#done manually, could use clustering in future to group chains referencing similar entities\n",
    "nodesfromchainID=dict()\n",
    "nodesfromchainID[\"Node_3\"]=list()\n",
    "nodesfromchainID[\"Node_3\"].extend([64,129,162,40,73,169,142,175,119,58])\n",
    "nodesfromchainID[\"Node_1\"]=list()\n",
    "nodesfromchainID[\"Node_1\"].extend([68,185])\n",
    "nodesfromchainID[\"Node_4\"]=list()\n",
    "nodesfromchainID[\"Node_4\"].extend([109,30])\n",
    "nodesfromchainID[\"Node_5\"]=list()\n",
    "nodesfromchainID[\"Node_5\"].extend([145])\n",
    "nodesfromchainID[\"Node_6\"]=list()\n",
    "nodesfromchainID[\"Node_6\"].extend([120])\n",
    "\n",
    "\n",
    "\n",
    "#hash map to replace all entities\n",
    "nodesID=dict()\n",
    "nodesID[\"us\"]=\"Node_1\"\n",
    "nodesID[\"you\"]=\"Node_2\"\n",
    "nodesID[\"woman\"]=\"Node_3\"\n",
    "nodesID[\"Bangladesh\"]=\"Node_4\"\n",
    "nodesID[\"Surbojoya\"]=\"Node_5\"\n",
    "nodesID[\"baby\"]=\"Node_6\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize \n",
    "\"\"\"original text\"\"\"\n",
    "org_text=text\n",
    "sentences=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  have signed . Let‚ us get to  ,  ! Mahfuz Islam started this petition to United Nations and  other \" SORBOJOYA \" is a project to assure women health service in Bangladesh . The purpose of SORBOJOYA is to provide safe menstrual health service to teenagers , tribal , Rohingya women and women of rural areas . Women plays a vital role towards our nation . Nine and a half crore population of Bangladesh is women which is more than half of the total population . So , the progress of the country is not possible by keeping this huge population behind . That\\'s the reason we need to focus on women\\'s health and need to keep our women\\'s safe . In Bangladesh still women are ashamed to discuss on their menstrual health . Women of rural areas , Rohingya women , tribal women are unaware of safe menstrual health service . They use clothes instead of sanitary napkins . Clothes are very unhygienic and unfriendly to skin . They do not have any idea about safe sanitation and menstrual health . Then women who have less earning or striving in their day to day life ca not afford a costly sanitary napkin . By this way women\\'s of our country derive themselves from safe sanitation . This ignorance of women towards their health results to maternal problems and diseases . Diseases like Prostate Cancer , Cysts , Uterus Cancer , etc takes place in women\\'s body . In Bangladesh  percent of the women affected by cancer sufferers from Prostate cancer . In each  minutes  woman gets affected by Prostate cancer and  women are identified with the symptoms of this cancer every year .  women dies each year due to this disease . It\\'s seen that most women dies are the sufferer of menstrual diseases . When a woman dies in a family gets affected , a society gets affected , moreover a nation gets affected . Day by day orphans are growing in our country due to the death of the mother . When a lactating mother dies , it also affects the baby . The baby is derived from having the milk and care from its mother . This affects the overall growth of the child too . Middle aged women or the teenagers suffers a lot due to this diseases . They need to leave their education half way . Some who gets married ca not adopt child . Women from different sectors lack on their day to day life . This scenario of the women\\'s of Bangladesh is so shacking . So being an developing country this phenomenon is not acceptable at all . So SORBOJOYA is trying to eliminate this unhygienic from the society . At present about  % women do not know about using sanitary napkin and  % of women knows about it but are not concerned . Only  % of the women can afford the sanitary napkins . Our aim is to reach every women of Bangladesh and educate them about safe menstrual health and the importance of using sanitary napkins during their period . If proper sanitation can be assured among the women then the percentage of diseases and the percentage of women dying due to menstrual disease can be lessened to  % . Our goal is to aware women about their health and make sanitary napkin available to them . By this we can save a huge population of our country which is a human resource . For this purpose we want your support to take this project further and make life better for all the women of Bangladesh . '"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9003 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-298a7e62eae540e2.props -preload lemma\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This is to store a dict of the reference string of an entity by sentence index\"\"\"\n",
    "node2=dict()\n",
    "node3=dict()\n",
    "node4=dict()\n",
    "node5=dict()\n",
    "node6=dict()\n",
    "node1=dict()\n",
    "for i in range(0,len(sentences)):\n",
    "    node3[i]=list()\n",
    "    node4[i]=list()\n",
    "    node5[i]=list()\n",
    "    node6[i]=list()\n",
    "    node1[i]=list()\n",
    "    node2[i]=list()\n",
    "\n",
    "for chain in chains:####text ###ann shouldnt be replaced\n",
    "    if chain.chainID in nodesfromchainID[\"Node_3\"]:\n",
    "        for mention in chain.mention:\n",
    "            words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
    "            ment_word = ' '.join([x.word for x in words_list])\n",
    "            node3[mention.sentenceIndex].append(ment_word)#word may contain some puncts\n",
    "    elif chain.chainID in nodesfromchainID[\"Node_1\"]:\n",
    "        for mention in chain.mention:\n",
    "            words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
    "            ment_word = ' '.join([x.word for x in words_list])\n",
    "            node1[mention.sentenceIndex].append(ment_word)#word may contain some puncts\n",
    "    elif chain.chainID in nodesfromchainID[\"Node_4\"]:\n",
    "        for mention in chain.mention:\n",
    "            words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
    "            ment_word = ' '.join([x.word for x in words_list])\n",
    "            node4[mention.sentenceIndex].append(ment_word)#word may contain some puncts\n",
    "    elif chain.chainID in nodesfromchainID[\"Node_5\"]:\n",
    "        for mention in chain.mention:\n",
    "            words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
    "            ment_word = ' '.join([x.word for x in words_list])\n",
    "            node5[mention.sentenceIndex].append(ment_word)#word may contain some puncts\n",
    "    elif chain.chainID in nodesfromchainID[\"Node_6\"]:\n",
    "        for mention in chain.mention:\n",
    "            words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
    "            ment_word = ' '.join([x.word for x in words_list])\n",
    "            node6[mention.sentenceIndex].append(ment_word)#word may contain some puncts\n",
    "            \n",
    "            \n",
    "editedsentences=sent_tokenize(NewText)\n",
    "\n",
    "\"\"\" Some words might represent the entities but may not be referenced\n",
    "to include such words -- match the lemma of the word to the entity dictionary lemma\"\"\"\n",
    "#second stage cleaning by lemma matching\n",
    "with CoreNLPClient(annotators=['lemma'], memory='4G',endpoint='http://localhost:9003') as client:\n",
    "    document = client.annotate(NewText)# since we don't want to include the words that are referenced already addressed\n",
    "    for i, sent in enumerate(document.sentence):\n",
    "        for t in sent.token:\n",
    "            if (t.lemma==\"we\"):\n",
    "                node1[i].append(t.word)\n",
    "                editedsentences[i]=editedsentences[i].replace(t.word,\"Node_1\")\n",
    "            elif (t.lemma==\"you\"):\n",
    "                node2[i].append(t.word)\n",
    "                editedsentences[i]=editedsentences[i].replace(t.word,\"Node_2\")\n",
    "            elif (t.lemma==\"woman\"):\n",
    "                node3[i].append(t.word)\n",
    "                editedsentences[i]=editedsentences[i].replace(t.word,\"Node_3\")\n",
    "            elif (t.lemma==\"country\"):\n",
    "                node4[i].append(t.word)\n",
    "                editedsentences[i]=editedsentences[i].replace(t.word,\"Node_4\")\n",
    "                \n",
    "NT=' '.join(editedsentences)\n",
    "editedsentences=sent_tokenize(NT)\n",
    "NewText=NT\n",
    "text=org_text# to undo any changes on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a half crore population of Bangladesh'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ment_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dependency parsing and POS tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9002 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-3b02b4be99fe408b.props -preload pos,depparse\n"
     ]
    }
   ],
   "source": [
    "# parts of speech tagging and dependency parsing\n",
    "with CoreNLPClient(annotators=['pos','depparse'], timeout=60000, memory='16G', endpoint='http://localhost:9002') as client:\n",
    "    ann = client.annotate(text)\n",
    "#    sentence=ann.sentence[2]\n",
    "#    dependency_parse = sentence.basicDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case(Nations-8,to-6)', 'compound(Nations-8,United-7)', 'conj(Nations-8,SORBOJOYA-12)', 'cc(SORBOJOYA-12,and-9)', 'amod(SORBOJOYA-12,other-10)', 'punct(SORBOJOYA-12,\"-11)', 'cop(project-16,is-14)', 'det(project-16,a-15)', 'acl(project-16,assure-18)', 'obl(assure-18,Bangladesh-23)', 'mark(assure-18,to-17)', 'obj(assure-18,service-21)', 'compound(service-21,women-19)', 'compound(service-21,health-20)', 'case(Bangladesh-23,in-22)', 'compound(Islam-2,Mahfuz-1)', 'obl(started-3,Nations-8)', 'punct(started-3,.-24)', 'punct(started-3,\"-13)', 'dep(started-3,project-16)', 'nsubj(started-3,Islam-2)', 'obj(started-3,petition-5)', 'det(petition-5,this-4)']\n"
     ]
    }
   ],
   "source": [
    "#result of dependency parsing of any sentence for ex here sentence 2\n",
    "sentence=ann.sentence[2]\n",
    "dp=sentence.basicDependencies\n",
    "res=list()\n",
    "for e in dp.edge:\n",
    "    s=e.dep+\"(\"+sentence.token[e.source-1].word+\"-\"+str(e.source)+\",\"+sentence.token[e.target-1].word+\"-\"+str(e.target)+\")\"\n",
    "    #print(e.dep,\"(\",sentence.token[e.source-1].word,\", \",sentence.token[e.target-1].word,\")\")\n",
    "    res.append(s)\n",
    "print(res)\n",
    "#parse is done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentiment scores entity pair wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "Node_3 Node_4\n",
      "[]\n",
      "Node_3 Node_5\n",
      "[]\n",
      "Node_4 Node_5\n",
      "[[('started', 'VBD'), ('Nations', 'NNP'), ('SORBOJOYA', 'NN'), ('project', 'NN'), ('assure', 'VB')]]\n",
      "[('started', 'VBD'), ('Nations', 'NNP'), ('SORBOJOYA', 'NN'), ('project', 'NN'), ('assure', 'VB')]\n",
      "3\n",
      "Node_3 Node_5\n",
      "[]\n",
      "4\n",
      "Node_1 Node_3\n",
      "[[]]\n",
      "[]\n",
      "5\n",
      "Node_3 Node_4\n",
      "[]\n",
      "6\n",
      "['Node_4']\n",
      "7\n",
      "8\n",
      "Node_3 Node_4\n",
      "[[('ashamed', 'JJ'), ('discuss', 'VB'), ('health', 'NN')], [('women', 'NNS'), ('ashamed', 'JJ')]]\n",
      "[('ashamed', 'JJ'), ('discuss', 'VB'), ('health', 'NN')]\n",
      "[('women', 'NNS'), ('ashamed', 'JJ')]\n",
      "9\n",
      "['Node_3']\n",
      "10\n",
      "['Node_3']\n",
      "11\n",
      "12\n",
      "['Node_3']\n",
      "13\n",
      "['Node_3']\n",
      "14\n",
      "Node_3 Node_4\n",
      "[[('women', 'NNS'), ('country', 'NN'), ('derive', 'VBP')], [('women', 'NNS'), ('country', 'NN'), ('derive', 'VBP')]]\n",
      "[('women', 'NNS'), ('country', 'NN'), ('derive', 'VBP')]\n",
      "[('women', 'NNS'), ('country', 'NN'), ('derive', 'VBP')]\n",
      "15\n",
      "['Node_3']\n",
      "16\n",
      "['Node_3']\n",
      "17\n",
      "Node_3 Node_4\n",
      "[]\n",
      "18\n",
      "['Node_3']\n",
      "19\n",
      "['Node_3']\n",
      "20\n",
      "['Node_3']\n",
      "21\n",
      "['Node_3']\n",
      "22\n",
      "Node_3 Node_4\n",
      "[[('growing', 'VBG'), ('country', 'NN'), ('death', 'NN'), ('mother', 'NN')]]\n",
      "[('growing', 'VBG'), ('country', 'NN'), ('death', 'NN'), ('mother', 'NN')]\n",
      "23\n",
      "['Node_6']\n",
      "24\n",
      "Node_3 Node_6\n",
      "[[('baby', 'NN'), ('derived', 'VBN'), ('having', 'VBG'), ('milk', 'NN'), ('mother', 'NN')]]\n",
      "[('baby', 'NN'), ('derived', 'VBN'), ('having', 'VBG'), ('milk', 'NN'), ('mother', 'NN')]\n",
      "25\n",
      "26\n",
      "['Node_3']\n",
      "27\n",
      "['Node_3']\n",
      "28\n",
      "29\n",
      "['Node_3']\n",
      "30\n",
      "['Node_3']\n",
      "31\n",
      "['Node_4']\n",
      "32\n",
      "['Node_5']\n",
      "33\n",
      "['Node_3']\n",
      "34\n",
      "['Node_3']\n",
      "35\n",
      "Node_1 Node_3\n",
      "[[('aim', 'NN'), ('reach', 'VB'), ('educate', 'VB'), ('health', 'NN'), ('importance', 'NN'), ('napkins', 'NNS'), ('period', 'NN')]]\n",
      "[('aim', 'NN'), ('reach', 'VB'), ('educate', 'VB'), ('health', 'NN'), ('importance', 'NN'), ('napkins', 'NNS'), ('period', 'NN')]\n",
      "Node_1 Node_4\n",
      "[[('aim', 'NN'), ('reach', 'VB'), ('women', 'NNS')]]\n",
      "[('aim', 'NN'), ('reach', 'VB'), ('women', 'NNS')]\n",
      "Node_3 Node_4\n",
      "[[('reach', 'VB'), ('women', 'NNS'), ('educate', 'VB')], [('reach', 'VB'), ('women', 'NNS'), ('educate', 'VB'), ('health', 'NN'), ('importance', 'NN'), ('napkins', 'NNS')], [('reach', 'VB'), ('women', 'NNS'), ('educate', 'VB'), ('period', 'NN')]]\n",
      "[('reach', 'VB'), ('women', 'NNS'), ('educate', 'VB')]\n",
      "[('reach', 'VB'), ('women', 'NNS'), ('educate', 'VB'), ('health', 'NN'), ('importance', 'NN'), ('napkins', 'NNS')]\n",
      "[('reach', 'VB'), ('women', 'NNS'), ('educate', 'VB'), ('period', 'NN')]\n",
      "36\n",
      "['Node_3']\n",
      "37\n",
      "Node_1 Node_3\n",
      "[[('goal', 'NN'), ('women', 'NNS'), ('health', 'NN'), ('make', 'VB'), ('available', 'JJ')]]\n",
      "[('goal', 'NN'), ('women', 'NNS'), ('health', 'NN'), ('make', 'VB'), ('available', 'JJ')]\n",
      "38\n",
      "39\n",
      "Node_1 Node_2\n",
      "[[]]\n",
      "[]\n",
      "Node_1 Node_3\n",
      "[[]]\n",
      "[]\n",
      "Node_1 Node_4\n",
      "[[('want', 'VBP'), ('take', 'VB'), ('make', 'VB'), ('better', 'JJR'), ('women', 'NNS')]]\n",
      "[('want', 'VBP'), ('take', 'VB'), ('make', 'VB'), ('better', 'JJR'), ('women', 'NNS')]\n",
      "Node_2 Node_3\n",
      "[]\n",
      "Node_2 Node_4\n",
      "[]\n",
      "Node_3 Node_4\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "PairSentimentDict_netCount=dict()\n",
    "PairSentimentDict_netCount[(\"Node_1\",\"Node_2\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_1\",\"Node_3\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_1\",\"Node_4\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_1\",\"Node_5\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_1\",\"Node_6\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_2\",\"Node_3\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_2\",\"Node_4\")]=0 \n",
    "PairSentimentDict_netCount[(\"Node_2\",\"Node_5\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_2\",\"Node_6\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_3\",\"Node_4\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_3\",\"Node_5\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_3\",\"Node_6\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_4\",\"Node_5\")]=0\n",
    "PairSentimentDict_netCount[(\"Node_5\",\"Node_6\")]=0\n",
    "PairSentimentDict_netsentiment=dict()\n",
    "PairSentimentDict_netsentiment[(\"Node_1\",\"Node_2\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_1\",\"Node_3\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_1\",\"Node_4\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_1\",\"Node_5\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_1\",\"Node_6\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_2\",\"Node_3\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_2\",\"Node_4\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_2\",\"Node_5\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_2\",\"Node_6\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_3\",\"Node_4\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_3\",\"Node_5\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_3\",\"Node_6\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_4\",\"Node_5\")]=0\n",
    "PairSentimentDict_netsentiment[(\"Node_5\",\"Node_6\")]=0\n",
    "#from nltk.tokenize import RegexpTokenizer \n",
    "#only words \n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
    "no_of_sentences=len(editedsentences)\n",
    "#no_of_sentences\n",
    "#sentences are sentence tokens\n",
    "for sentID in range(0,no_of_sentences):\n",
    "    print(sentID)\n",
    "    if sentID!=7:# was hanging due to statement 7 so skipped it\n",
    "        l=uniqueentitylist(editedsentences[sentID],entity_keys)\n",
    "        if(len(l)==1 and l[0] != 'Node_1'):\n",
    "            1\n",
    "            print(l) \n",
    "        else:\n",
    "            ent_pairs=entity_pairs_in_sent(l)\n",
    "            for tup in ent_pairs:\n",
    "                (x,y)=tup\n",
    "                print(x,y)\n",
    "                listofpairDepPhrase=findDependencyPhrase(sentID,x,y,ann)\n",
    "                print(listofpairDepPhrase)\n",
    "                for DepPhrase in listofpairDepPhrase:\n",
    "                    print(DepPhrase)\n",
    "                    PairSentimentDict_netsentiment[tup]=PairSentimentDict_netsentiment[tup]+list_to_sentiment(DepPhrase)\n",
    "                    PairSentimentDict_netCount[tup]=PairSentimentDict_netCount[tup]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node_1 Node_3 : 0.07124068061568062\n",
      "Node_1 Node_4 : 0.13228039321789323\n",
      "Node_3 Node_4 : -0.10892030423280426\n",
      "Node_3 Node_6 : 0.002751018170426065\n",
      "Node_4 Node_5 : 0.06398809523809525\n"
     ]
    }
   ],
   "source": [
    "#sentiment scores\n",
    "\n",
    "\n",
    "for tup in PairSentimentDict_netsentiment.keys():\n",
    "    (x,y)=tup\n",
    "    if PairSentimentDict_netsentiment[tup]!=0:\n",
    "        s=PairSentimentDict_netsentiment[tup]\n",
    "        n=PairSentimentDict_netCount[tup]\n",
    "        pairwiseSentiment=s/n\n",
    "        print(x,y,\":\",pairwiseSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Node_1', 'Node_2'): 1,\n",
       " ('Node_1', 'Node_3'): 4,\n",
       " ('Node_1', 'Node_4'): 2,\n",
       " ('Node_1', 'Node_5'): 0,\n",
       " ('Node_1', 'Node_6'): 0,\n",
       " ('Node_2', 'Node_3'): 0,\n",
       " ('Node_2', 'Node_4'): 0,\n",
       " ('Node_2', 'Node_5'): 0,\n",
       " ('Node_2', 'Node_6'): 0,\n",
       " ('Node_3', 'Node_4'): 8,\n",
       " ('Node_3', 'Node_5'): 0,\n",
       " ('Node_3', 'Node_6'): 1,\n",
       " ('Node_4', 'Node_5'): 1,\n",
       " ('Node_5', 'Node_6'): 0}"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of number of pair occurences\n",
    "PairSentimentDict_netCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'us': 'Node_1',\n",
       " 'you': 'Node_2',\n",
       " 'woman': 'Node_3',\n",
       " 'Bangladesh': 'Node_4',\n",
       " 'Surbojoya': 'Node_5',\n",
       " 'baby': 'Node_6'}"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dict for meaning of entity key values\n",
    "\n",
    "nodesID\n",
    "#pair wise sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
